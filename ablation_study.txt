Overview
We developed two pretraining-based solutions, one
utilizing GloVe embeddings and the other incorporating both GloVe and BERT.

Techniques Used
- data preprocessing
- tokenization and GloVe/BERT for embedding
- classification model designed to predict the correctness of long answer candidates
- model that predicts the start and end indices of short answers
- evaluation metrics: accuracy, recall, precision, and F1 score
- submission: .csv file containing predicted long and short answer tokens for submission

For detailed implementation and results, refer to the comprehensive report.

Kaggle notebook - GloVe
https://www.kaggle.com/code/irinarebegea/atnn-qa-pretrained-glove

Competition score: 0.12


Kaggle notebook - BERT
https://www.kaggle.com/code/irinarebegea/atnn-question-answering-bert

Competition score: -